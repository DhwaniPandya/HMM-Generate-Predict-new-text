{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Viterbi Algorithm implementation on Amazon product review data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented Viterbi algorithm for text predection on  Amazon reviews dataset.\n",
    "Hidden/Transition states are = Words in the review(Considered unique word set as states)\n",
    "\n",
    "Transition probability (one state (word) to other word)\n",
    "\n",
    "Emission probability :Every review is our observation. According, the thoeryof HMM We have Sentences/reviews provided so that we can uses as observations and calculate the probability of different states/words from this observations.\n",
    "\n",
    "Initial probability : Occurace of a word/state in total no.of states/word.\n",
    "Transition probability : Probabilty(word1|word2)\n",
    "Emission Probability : Probability (word/no.of words in the passage)*normallization value\n",
    "Viterbi Porbability = (viterbi_probability of last entry)*(Transion probability of state)*(Emission probability for sate to observation)\n",
    "\n",
    "Viterbi sequence formation  with max probabilites at each step \n",
    "Backtrac sequece : a dictionary of max probability as values and predicted state)\n",
    "\n",
    "Text Prediction is done using HMM Viterbi algorithm. Time span is noted down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Required libraires for the project\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "df = pd.read_csv('Reviews.csv',sep=',',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      "Id                        568454 non-null int64\n",
      "ProductId                 568454 non-null object\n",
      "UserId                    568454 non-null object\n",
      "ProfileName               568438 non-null object\n",
      "HelpfulnessNumerator      568454 non-null int64\n",
      "HelpfulnessDenominator    568454 non-null int64\n",
      "Score                     568454 non-null int64\n",
      "Time                      568454 non-null int64\n",
      "Summary                   568427 non-null object\n",
      "Text                      568454 non-null object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#Main data file\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Created a new data set from base one\n",
    "df1= pd.DataFrame(df['Text'][0:8000],columns=['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8000 entries, 0 to 7999\n",
      "Data columns (total 1 columns):\n",
      "Text    8000 non-null object\n",
      "dtypes: object(1)\n",
      "memory usage: 62.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Used Natual language processing tool kit to word tokenize the data\n",
    "from nltk import word_tokenize\n",
    "df1['Text'].dropna(inplace=True)\n",
    "df1['tokenized_text'] =df1['Text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [I, have, bought, several, of, the, Vitality, ...\n",
       "1    [Product, arrived, labeled, as, Jumbo, Salted,...\n",
       "2    [This, is, a, confection, that, has, been, aro...\n",
       "3    [If, you, are, looking, for, the, secret, ingr...\n",
       "4    [Great, taffy, at, a, great, price, ., There, ...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'have',\n",
       " 'bought',\n",
       " 'several',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Vitality',\n",
       " 'canned',\n",
       " 'dog',\n",
       " 'food',\n",
       " 'products',\n",
       " 'and',\n",
       " 'have',\n",
       " 'found',\n",
       " 'them',\n",
       " 'all',\n",
       " 'to',\n",
       " 'be',\n",
       " 'of',\n",
       " 'good',\n",
       " 'quality',\n",
       " '.',\n",
       " 'The',\n",
       " 'product',\n",
       " 'looks',\n",
       " 'more',\n",
       " 'like',\n",
       " 'a',\n",
       " 'stew',\n",
       " 'than',\n",
       " 'a',\n",
       " 'processed',\n",
       " 'meat',\n",
       " 'and',\n",
       " 'it',\n",
       " 'smells',\n",
       " 'better',\n",
       " '.',\n",
       " 'My',\n",
       " 'Labrador',\n",
       " 'is',\n",
       " 'finicky',\n",
       " 'and',\n",
       " 'she',\n",
       " 'appreciates',\n",
       " 'this',\n",
       " 'product',\n",
       " 'better',\n",
       " 'than',\n",
       " 'most',\n",
       " '.']"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenzied output sample\n",
    "df1['tokenized_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prashanthimallijula/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Check stop words and special characters\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "special=['!','@','#','$','%','^','&','*','(',')',':','<','>',',','.',';','{','}','|','_','-','+','=','`','~',\"--\",\"'\",'\"','[',']']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove speacial characters\n",
    "for item  in df1['tokenized_text']:\n",
    "    item_new=[]\n",
    "    for word in item:\n",
    "        word=str(word)\n",
    "        if (word in special):\n",
    "            item.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>[I, have, bought, several, of, the, Vitality, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>[Product, arrived, labeled, as, Jumbo, Salted,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>[This, is, a, confection, that, has, been, aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>[If, you, are, looking, for, the, secret, ingr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>[Great, taffy, at, a, great, price, There, was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  I have bought several of the Vitality canned d...   \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  This is a confection that has been around a fe...   \n",
       "3  If you are looking for the secret ingredient i...   \n",
       "4  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [I, have, bought, several, of, the, Vitality, ...  \n",
       "1  [Product, arrived, labeled, as, Jumbo, Salted,...  \n",
       "2  [This, is, a, confection, that, has, been, aro...  \n",
       "3  [If, you, are, looking, for, the, secret, ingr...  \n",
       "4  [Great, taffy, at, a, great, price, There, was...  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create trasition states\n",
    "trans_states=[]\n",
    "for item  in df1['tokenized_text']:\n",
    "        temp_list = []\n",
    "        for i in range(0,len(item)) :\n",
    "            #if i+1 == len(item):\n",
    "                    #temp_list.append((item[i],\"end\"))\n",
    "            if i+1 < len(item):\n",
    "                    temp_list.append((item[i],item[i+1]))\n",
    "        trans_states.append(temp_list)\n",
    "df1['trans_states'] =trans_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>trans_states</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>[I, have, bought, several, of, the, Vitality, ...</td>\n",
       "      <td>[(I, have), (have, bought), (bought, several),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>[Product, arrived, labeled, as, Jumbo, Salted,...</td>\n",
       "      <td>[(Product, arrived), (arrived, labeled), (labe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>[This, is, a, confection, that, has, been, aro...</td>\n",
       "      <td>[(This, is), (is, a), (a, confection), (confec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>[If, you, are, looking, for, the, secret, ingr...</td>\n",
       "      <td>[(If, you), (you, are), (are, looking), (looki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>[Great, taffy, at, a, great, price, There, was...</td>\n",
       "      <td>[(Great, taffy), (taffy, at), (at, a), (a, gre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  \\\n",
       "0  I have bought several of the Vitality canned d...   \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  This is a confection that has been around a fe...   \n",
       "3  If you are looking for the secret ingredient i...   \n",
       "4  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [I, have, bought, several, of, the, Vitality, ...   \n",
       "1  [Product, arrived, labeled, as, Jumbo, Salted,...   \n",
       "2  [This, is, a, confection, that, has, been, aro...   \n",
       "3  [If, you, are, looking, for, the, secret, ingr...   \n",
       "4  [Great, taffy, at, a, great, price, There, was...   \n",
       "\n",
       "                                        trans_states  \n",
       "0  [(I, have), (have, bought), (bought, several),...  \n",
       "1  [(Product, arrived), (arrived, labeled), (labe...  \n",
       "2  [(This, is), (is, a), (a, confection), (confec...  \n",
       "3  [(If, you), (you, are), (are, looking), (looki...  \n",
       "4  [(Great, taffy), (taffy, at), (at, a), (a, gre...  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a dictionary of transtion states and corresponding probabilites\n",
    "trans_states={}\n",
    "dict_trans_states=[]\n",
    "for item  in df1['trans_states']:\n",
    "    c=0\n",
    "    c1=0\n",
    "    trans_states={}\n",
    "    for i in item :\n",
    "        c=0\n",
    "        c1=0\n",
    "        for j in item:\n",
    "            if i == j:\n",
    "                c= c+1\n",
    "            if i[0]==j[0]:\n",
    "                c1=c1+1\n",
    "        trans_states[i]=c/c1\n",
    "    dict_trans_states.append(trans_states)\n",
    "df1['dict_trans_states'] =dict_trans_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert it as data frame\n",
    "trans_dict={}\n",
    "for item in df1['dict_trans_states']:\n",
    "    for key,value in item.items():\n",
    "        trans_dict[key]=value\n",
    "\n",
    "trans_df = pd.DataFrame(trans_dict.items())       \n",
    "trans_df.columns=['trans_entry','trans_Probability']\n",
    "trans_df['trans_entry1']=trans_df['trans_entry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_df.set_index('trans_entry', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get uniques states in the data that is unique words in our case\n",
    "unique_states=[]\n",
    "temp_list=[]\n",
    "for item in df1['tokenized_text']:\n",
    "    for word in item:\n",
    "        temp_list.append(word)\n",
    "temp_list =set(temp_list)\n",
    "unique_states=temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sentences Lower case\n",
    "df1['text_lowercase']= df1['Text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('I', 'have'): 1.0,\n",
       " ('Labrador', 'is'): 1.0,\n",
       " ('My', 'Labrador'): 1.0,\n",
       " ('The', 'product'): 1.0,\n",
       " ('Vitality', 'canned'): 1.0,\n",
       " ('a', 'processed'): 0.5,\n",
       " ('a', 'stew'): 0.5,\n",
       " ('all', 'to'): 1.0,\n",
       " ('and', 'have'): 0.3333333333333333,\n",
       " ('and', 'it'): 0.3333333333333333,\n",
       " ('and', 'she'): 0.3333333333333333,\n",
       " ('appreciates', 'this'): 1.0,\n",
       " ('be', 'of'): 1.0,\n",
       " ('better', 'My'): 0.5,\n",
       " ('better', 'than'): 0.5,\n",
       " ('bought', 'several'): 1.0,\n",
       " ('canned', 'dog'): 1.0,\n",
       " ('dog', 'food'): 1.0,\n",
       " ('finicky', 'and'): 1.0,\n",
       " ('food', 'products'): 1.0,\n",
       " ('found', 'them'): 1.0,\n",
       " ('good', 'quality'): 1.0,\n",
       " ('have', 'bought'): 0.5,\n",
       " ('have', 'found'): 0.5,\n",
       " ('is', 'finicky'): 1.0,\n",
       " ('it', 'smells'): 1.0,\n",
       " ('like', 'a'): 1.0,\n",
       " ('looks', 'more'): 1.0,\n",
       " ('meat', 'and'): 1.0,\n",
       " ('more', 'like'): 1.0,\n",
       " ('of', 'good'): 0.5,\n",
       " ('of', 'the'): 0.5,\n",
       " ('processed', 'meat'): 1.0,\n",
       " ('product', 'better'): 0.5,\n",
       " ('product', 'looks'): 0.5,\n",
       " ('products', 'and'): 1.0,\n",
       " ('quality', 'The'): 1.0,\n",
       " ('several', 'of'): 1.0,\n",
       " ('she', 'appreciates'): 1.0,\n",
       " ('smells', 'better'): 1.0,\n",
       " ('stew', 'than'): 1.0,\n",
       " ('than', 'a'): 0.5,\n",
       " ('than', 'most'): 0.5,\n",
       " ('the', 'Vitality'): 1.0,\n",
       " ('them', 'all'): 1.0,\n",
       " ('this', 'product'): 1.0,\n",
       " ('to', 'be'): 1.0}"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Tansition probability output\n",
    "df1['dict_trans_states'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculation of Term frequency and used as emiziion probability and \n",
    "#TDIDF will be used in many case to check the importance of the word \n",
    "def tf(item):\n",
    "    tf_numerator= item[1].count(item[0])\n",
    "    tf_denominator = len(item[1].split())\n",
    "    TF=tf_numerator/float(tf_denominator)\n",
    "    return (TF)\n",
    "emission_dict={}\n",
    "temp_l=[]\n",
    "i=0\n",
    "for word in unique_states:\n",
    "        for item in df1['text_lowercase']:\n",
    "            if word in item: \n",
    "                temp_l = [word,item]\n",
    "                key=tuple(temp_l)\n",
    "                TF=tf(key)\n",
    "                emission_dict[key] = float(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('week.', \"one of my boys needed to lose some weight and the other didn't.  i put this food on the floor for the chubby guy, and the protein-rich, no by-product food up higher where only my skinny boy can jump.  the higher food sits going stale.  they both really go for this food.  and my chubby boy has been losing about an ounce a week.\")\n",
      " \n",
      "0.015625\n",
      "('week.', 'use frequently as we like to do asian dishes at least once a week.  love this product.  fast shipping, as usual.  would buy again.')\n",
      " \n",
      "0.041666666666666664\n",
      "('week.', \"i am a preschool teacher and do a g is for gingerbread unit every january. i was on vacation during christmas and when i got back not a gingerbread kit was to be found. which is odd. i needed one for my preschool class and needed it in less then a week. i looked everywhere and could not find ont. so i had one overnighted from amazon on nye and it came yesterday. i pick this one because of the rudolph theme. the gingerbread itself came broken. like unable to use would need a glue gun to get it to work broken. it's non returnable. i called amazon, they were nice enough to order me a second one since i spend a ton of money each year with them. this one also came broken. i order a lot from amazon and things do not come broken. i think only one time have i had returned something because it was broken in 100's of orders. but two of the same thing in a row?! i think it's the kit itself and nothing to do with how it was shipped. i will say most come ups and overnight comes fed ed. but it's bubble wrapped very well. i think the gingerbread isn't good and cracks in shipping and breaks off. the rest of the kit is very cute. the first kit the candies had spilled out and where a mess in the box. but the second kit while the gingerbread was all broken up again the candies were all in their bags. too bad. but i would try something else, stay away from this one.\")\n",
      " \n",
      "0.003663003663003663\n",
      "('week.', 'this hot sauce is one of my favorites. its a perfect balance of tasty and spicy. you can smell the flavors immediately when you open the bottle. a mix of garlic, habanero, and other spices. i put this on everything and tend to go through one bottle a week. i highly recommend this to anyone who likes hot sauce.')\n",
      " \n",
      "0.01694915254237288\n",
      "('week.', \"january 3, 2011<br /><br />i purchased these potatoes from omaha steaks online last week. i was not sure what to expect ... the au gratin potatoes i had seen and tasted before were sliced potatoes with globbs of gooey cheese sauce, and by the looks of them, this certainly wasn't it ... i opened the box and got out my skillet. the first thing that i noticed is that these things were huge ... like a gigantic tater tot. i browned them on both sides about 10 minutes each. to my surprise, they were very tasty. texture was light fluffy and the potatoes tasted extra fresh .. not frozen or freezer burned. i liked them alot. i think these are perfect for kids ... none of the flavors were overpowering and great texture ... i give these 4 stars.\")\n",
      " \n",
      "0.007194244604316547\n",
      "('week.', \"this is my favorite waffle mix (it can be used to make pancakes too, but you have to add milk, and we don't usually have milk in the house).  you have to add water, 1 egg, and melted butter for the waffle recipe.  we eat waffles about once a week.<br /><br />apart from the fact that it tastes great (it has a little vanilla and malt mixed in), it has no trans fat.  some pancake/waffle mixes, such as krustead's which also tastes really good, say they have no trans fat per serving, but if it has partially hydrogenated vegetable oil in the ingredients, there is still some (less than than 1 percent) trans fat per serving.  stonewall has none.<br /><br />we used to have a store that sold it locally, but now we don't so i am glad we can get it online.<br /><br />the can comes with a recipe that is perfect for making two belgian waffles with no leftover batter.<br /><br />we just bought the regular mix but we are going to try the other flavors next.\")\n",
      " \n",
      "0.00558659217877095\n"
     ]
    }
   ],
   "source": [
    "#sample Emission probability dictionary values Different observations and one state\n",
    "i=0\n",
    "for key,value in emission_dict.items():\n",
    "    if i<=5:\n",
    "        print(key)\n",
    "        print(\" \")\n",
    "        print(emission_dict[key])\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Preparing inital probability for all states\n",
    "initial_prob={}\n",
    "for word1 in unique_states:\n",
    "    c=0\n",
    "    for word2 in unique_states:\n",
    "        if word1==word2:\n",
    "              c=c+1\n",
    "    initial_prob[word1]=c/len(unique_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Converting it to data frame for better usage\n",
    "init_df = pd.DataFrame(initial_prob.items())\n",
    "init_df.columns=['init_entry','init_Probability']\n",
    "init_df.set_index('init_entry', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>init_Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>init_entry</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>week.</th>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infant</th>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anticipating</th>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foils</th>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>murky</th>\n",
       "      <td>0.000038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              init_Probability\n",
       "init_entry                    \n",
       "week.                 0.000038\n",
       "infant                0.000038\n",
       "anticipating          0.000038\n",
       "foils                 0.000038\n",
       "murky                 0.000038"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample initial probabilites\n",
    "init_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6741\n",
      "i usually cringe a little before i do a natural juice/carbonated beverage.  there is frequently something of an aftertaste that is unpleasant.<br /><br />this is refreshing, light, very bubbly and leaves a bright taste after with no cringe effects.<br /><br />i definitely would buy this drink.\n"
     ]
    }
   ],
   "source": [
    "# seed the pseudo random number generator .  A randome number picke to get a random paragraph to work on \n",
    "from random import randint\n",
    "# seed random number generator\n",
    "rand_int = randint(0, 7999)\n",
    "print(rand_int)\n",
    "print(df1['text_lowercase'][rand_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i usually cringe a little before i do a natural juice/carbonated beverage.  there is frequently something of an aftertaste that is unpleasant.<br /><br />this is refreshing, light, very bubbly and leaves a bright taste after with no cringe effects.<br /><br />i definitely would buy this drink.\n"
     ]
    }
   ],
   "source": [
    "random_list=[]\n",
    "random_list= df1['tokenized_text'][rand_int]\n",
    "text=df1['text_lowercase'][rand_int]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Somethods to extract Emission probability, Transition probability, Initial probability and states list for the paragraph\n",
    "def get_obs_text(cur_state):\n",
    "    list1=[]\n",
    "    for key1,value1 in emission_dict.items():\n",
    "        if  key1[0] == cur_state:\n",
    "            list1.append(float(value1))\n",
    "        else:\n",
    "            list1.append(0)\n",
    "    considered_obs=max(list1)\n",
    "    for key2,value2 in emission_dict.items():\n",
    "          if value2 == considered_obs:\n",
    "                return(key)\n",
    "\n",
    "def get_required_states(text):  \n",
    "    states_list=[]\n",
    "    obs_list=[]\n",
    "    for key,value in emission_dict.items():\n",
    "            if (len(key[0])>3) and (key[1] == text):\n",
    "                states_list.append(key[0])\n",
    "\n",
    "    states_list = set(states_list)\n",
    "    states_list=list(states_list)\n",
    "    for i in states_list:\n",
    "        for item in df1['text_lowercase']:\n",
    "            if i in item:\n",
    "                obs_list.append(item)\n",
    "    obs_list=set(obs_list)\n",
    "    obs_list=list(obs_list)\n",
    "    return(states_list,obs_list)\n",
    "\n",
    "\n",
    "def get_init_prob(word):\n",
    "    if init_df.loc[word,'init_Probability'] !=0:\n",
    "        init_prob= init_df.loc[word,'init_Probability'] #Get the initial probability of the word\n",
    "    else:\n",
    "        init_prob=0 \n",
    "    return init_prob\n",
    "def get_em_prob(temp_tuple):\n",
    "    if emission_dict[temp_tuple] !=0:\n",
    "         v_prob=emission_dict[temp_tuple]\n",
    "    else:\n",
    "        v_prob=0\n",
    "    return v_prob\n",
    "\n",
    "def trans_prob(st1,st2):\n",
    "    word=[st1,st2]\n",
    "    word=tuple(word)\n",
    "    if trans_dict[word] !=0:\n",
    "        trans_prob= trans_dict[word]\n",
    "    else:\n",
    "        trans_prob=1 \n",
    "    return(trans_prob)\n",
    "\n",
    "\n",
    "def get_req_trans_states(state):\n",
    "    req_trans_states=[]\n",
    "    tr=[]\n",
    "    for i in trans_df['trans_entry1']:\n",
    "        if (i[0] == state) :\n",
    "            req_trans_states.append(i[1])\n",
    "            tr.append(i)\n",
    "    return(req_trans_states,tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi probability seqence:\n",
      "\n",
      "[1.6325571068475976e-06, 2.0744054724874177e-09, 2.635839228065334e-12, 3.3492239238441347e-15, 4.255684782521137e-18, 5.407477487320378e-21, 6.871000619212678e-24, 8.730623404336314e-27, 1.1093549433718314e-29, 1.409599673915923e-32, 1.791105049448441e-35]\n",
      "\n",
      "Track path:\n",
      "\n",
      "appetizing : 5.867002102733554e-07\n",
      "Once : 7.454894666751657e-10\n",
      "you : 9.472547225859793e-13\n",
      "buying : 1.2036273476314858e-15\n",
      "this : 1.5293867187185336e-18\n",
      "product : 1.943312222005761e-21\n",
      "looks : 2.4692658475295565e-24\n",
      "more : 3.137567785933363e-27\n",
      "than : 3.9867443277425194e-30\n",
      "most : 5.065748828135349e-33\n",
      "have : 6.436783771455335e-36\n",
      "like : 7.509762691498949e-07\n",
      "cool : 5.074163980742533e-07\n",
      "place : 6.447476468541974e-10\n",
      "because : 8.192473276419282e-13\n",
      "they : 1.0409750033569608e-15\n",
      "were : 1.3227128378106236e-18\n",
      "stuck : 1.6807024622752525e-21\n",
      "together : 2.1355812735390755e-24\n",
      "which : 2.7135721391856105e-27\n",
      "was : 3.4479950942638e-30\n",
      "made : 4.381188175684625e-33\n",
      "some : 5.566948126664073e-36\n",
      "caffein : 4.876469280194123e-07\n",
      "moth : 1.2112520470159595e-06\n",
      "ring : 1.6325571068475976e-06\n",
      "with : 8.730623404336314e-27\n",
      "powdered : 1.1093549433718314e-29\n",
      "sugar : 1.409599673915923e-32\n",
      "tastes : 1.791105049448441e-35\n",
      "better : 5.407477487320378e-21\n",
      "one : 6.871000619212678e-24\n",
      "\n",
      "Input text(Random picked)\n",
      "\n",
      "i usually cringe a little before i do a natural juice/carbonated beverage.  there is frequently something of an aftertaste that is unpleasant.<br /><br />this is refreshing, light, very bubbly and leaves a bright taste after with no cringe effects.<br /><br />i definitely would buy this drink.\n",
      "  \n",
      "continued with Predicted text...\n",
      "Predicted text:\n",
      "  \n",
      "['appetizing', 'Once', 'you', 'buying', 'this', 'product', 'looks', 'more', 'than', 'most', 'have', 'like', 'cool', 'place', 'because', 'they', 'were', 'stuck', 'together', 'which', 'was', 'made', 'some', 'caffein', 'moth', 'ring', 'with', 'powdered', 'sugar', 'tastes', 'better', 'one']\n",
      "  \n",
      "combined with input\n",
      "i usually cringe a little before i do a natural juice/carbonated beverage.  there is frequently something of an aftertaste that is unpleasant.<br /><br />this is refreshing, light, very bubbly and leaves a bright taste after with no cringe effects.<br /><br />i definitely would buy this drink.\n",
      "['appetizing', 'Once', 'you', 'buying', 'this', 'product', 'looks', 'more', 'than', 'most', 'have', 'like', 'cool', 'place', 'because', 'they', 'were', 'stuck', 'together', 'which', 'was', 'made', 'some', 'caffein', 'moth', 'ring', 'with', 'powdered', 'sugar', 'tastes', 'better', 'one']\n",
      "  \n",
      "Program Complted\n"
     ]
    }
   ],
   "source": [
    "#Starting HMM VITERBI ALGORITHM FROM HERE ,FUNCTIONS REQUIRED ARE DEFINE ABOVE\n",
    "def viterbi_algo(text,states_list,obs_list,trans_state_list,t,state,viterbi_prob_sequence,backtrack_states):\n",
    "    #Initial probabilites of inital states considering corresponding observation.\n",
    "    while (t<10):\n",
    "        temp_v_dict={}\n",
    "        #print(t)\n",
    "        if t==0:\n",
    "            v_init_prob=get_init_prob(state)\n",
    "            temp_list=[state,text]\n",
    "            temp_tuple= tuple(temp_list)\n",
    "            v_init_emision= get_em_prob(temp_tuple)\n",
    "            v_entry= v_init_prob*v_init_emision\n",
    "            viterbi_prob_sequence.append(v_entry)\n",
    "            backtrack_states[state]= v_entry\n",
    "        for st in trans_state_list:\n",
    "                if (len(st)>=3) and (st.isalpha()==True):\n",
    "                    #print(\"st\",st)\n",
    "                    text1=get_obs_text(st.lower())\n",
    "                    if text1 != None:\n",
    "                        v_emision_prob=get_em_prob(text1)\n",
    "                        v_trans_prob= trans_prob(state,st)\n",
    "                        v_entry= viterbi_prob_sequence[-1]*v_emision_prob*v_trans_prob\n",
    "                        temp_v_dict[st]=v_entry\n",
    "        max_prob=max(temp_v_dict.values())\n",
    "        viterbi_prob_sequence.append(max_prob)\n",
    "        i=0\n",
    "        for k,v in temp_v_dict.items():\n",
    "            if (v==max_prob) and (i==0):\n",
    "                backtrack_states[k]= max_prob\n",
    "                #print(k,v)\n",
    "                new_state=k\n",
    "                i=i+1\n",
    "    \n",
    "        #calculate next level list\n",
    "        new_text=get_obs_text(new_state)\n",
    "        states_list,obs_list = get_required_states(new_text)\n",
    "        trans_state_list,tr = get_req_trans_states(new_state)\n",
    "        t=t+1\n",
    "        #print(\"here:\", t)\n",
    "        if t<10:\n",
    "            viterbi_algo(new_text,states_list,obs_list,trans_state_list,t,new_state,viterbi_prob_sequence,backtrack_states)\n",
    "        return (viterbi_prob_sequence,backtrack_states)\n",
    "\n",
    "# Viterbi sequence and backtrack calculation\n",
    "viterbi_prob_sequence=[] \n",
    "backtrack_state={}\n",
    "\n",
    "\n",
    "#Extracting required states and observations for the picked random corpus\n",
    "states_list,obs_list = get_required_states(text)\n",
    "state = states_list[-1]\n",
    "trans_state_list,tr = get_req_trans_states(state)\n",
    "#Limited no.of iterations to predict only to words after the end.\n",
    "t=0\n",
    "viterbi_prob_sequence,backtrack_states= viterbi_algo(text,states_list,obs_list,trans_state_list,t,state,viterbi_prob_sequence,backtrack_states)\n",
    "value_list=[]\n",
    "#Print the output\n",
    "print('Viterbi probability seqence:')\n",
    "print(\"\")\n",
    "print(viterbi_prob_sequence)\n",
    "print(\"\")\n",
    "print(\"Track path:\")\n",
    "print(\"\")\n",
    "for key,value in backtrack_states.items():\n",
    "        print (key ,\":\", value)\n",
    "        value_list.append(key)\n",
    "print(\"\")\n",
    "print(\"Input text(Random picked)\")\n",
    "print(\"\")\n",
    "print(text)\n",
    "print(\"  \")\n",
    "print(\"continued with Predicted text...\" )\n",
    "print(\"Predicted text:\" )\n",
    "print(\"  \")\n",
    "print (value_list)\n",
    "print(\"  \")\n",
    "print(\"combined with input\")\n",
    "print(text)\n",
    "print (value_list)\n",
    "print(\"  \")\n",
    "print(\"Program Complted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
